{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SYkAbHbfcku-"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2L_yKE1IgpA0"
   },
   "outputs": [],
   "source": [
    "#%autoreload # When utils.py is updated\n",
    "from utils_unet_resunet import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from model.models import Model_3\n",
    "root_path = 'imgs/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kylt2BueckvP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs/New_Images/Sentinel2/2018_10m_b2348.tif\n",
      "imgs/New_Images/Sentinel2/2019_10m_b2348.tif\n",
      "imgs/New_Images/Sentinel1/cut_sent1_vh_2018.tif\n",
      "imgs/New_Images/Sentinel1/cut_sent1_vv_2018.tif\n",
      "imgs/New_Images/Sentinel1/cut_sent1_vh_2019.tif\n",
      "imgs/New_Images/Sentinel1/cut_sent1_vv_2019.tif\n",
      "Image stack: (17729, 9202, 12)\n"
     ]
    }
   ],
   "source": [
    "# Define data type (L8-Landsat8, S2-Sentinel2, S1-Sentinel1)\n",
    "img_type = 'FUSION'\n",
    "\n",
    "if img_type == 'L8':\n",
    "    # Load images\n",
    "    ref_2019 = load_tif_image(root_path+'New_Images/References/res_10m/r10m_def_2019.tif').astype('float32') # actual 2019\n",
    "    opt_2018 = load_tif_image(root_path+'New_Images/Landsat8/'+'cut_land8_2018.tif').astype('float32')   \n",
    "    opt_2019 = load_tif_image(root_path+'New_Images/Landsat8/'+'cut_land8_2019.tif').astype('float32')\n",
    "\n",
    "    # Resize images\n",
    "    opt_2018 = resize_image(opt_2018.copy(), ref_2019.shape[0], ref_2019.shape[1])\n",
    "    opt_2019 = resize_image(opt_2019.copy(), ref_2019.shape[0], ref_2019.shape[1])  \n",
    "\n",
    "    # Filter outliers\n",
    "    opt_2018 = filter_outliers(opt_2018.copy()) \n",
    "    opt_2019 = filter_outliers(opt_2019.copy())\n",
    "    \n",
    "    image_stack = np.concatenate((opt_2018, opt_2019), axis=-1)\n",
    "    print('landsat_resize:', image_stack.shape)\n",
    "    del opt_2018, opt_2019\n",
    "\n",
    "if img_type == 'S2':\n",
    "    # Load images\n",
    "    sent2_2018_1 = load_tif_image(root_path+'New_Images/Sentinel2/'+'2018_10m_b2348.tif').astype('float32')\n",
    "    #sent2_2018_2 = load_tif_image(root_path+'New_Images/Sentinel2/'+'2018_20m_b5678a1112.tif').astype('float32')\n",
    "    \n",
    "    # Resize bands of 20m\n",
    "    #sent2_2018_2 = resize_image(sent2_2018_2.copy(), sent2_2018_1.shape[0], sent2_2018_1.shape[1])\n",
    "    #sent2_2018 = np.concatenate((sent2_2018_1, sent2_2018_2), axis=-1)\n",
    "    sent2_2018 = sent2_2018_1.copy()\n",
    "    del sent2_2018_1#, sent2_2018_2\n",
    "    \n",
    "    sent2_2019_1 = load_tif_image(root_path+'New_Images/Sentinel2/'+'2019_10m_b2348.tif').astype('float32')\n",
    "    #sent2_2019_2 = load_tif_image(root_path+'New_Images/Sentinel2/'+'2019_20m_b5678a1112.tif').astype('float32')   \n",
    "    \n",
    "    # Resize bands of 20m\n",
    "    #sent2_2019_2 = resize_image(sent2_2019_2.copy(), sent2_2019_1.shape[0], sent2_2019_1.shape[1])\n",
    "    #sent2_2019 = np.concatenate((sent2_2019_1, sent2_2019_2), axis=-1)\n",
    "    sent2_2019 = sent2_2019_1.copy()\n",
    "    del sent2_2019_1#, sent2_2019_2\n",
    "    \n",
    "    # Filter outliers\n",
    "    sent2_2018 = filter_outliers(sent2_2018.copy()) \n",
    "    sent2_2019 = filter_outliers(sent2_2019.copy()) \n",
    "    \n",
    "    image_stack = np.concatenate((sent2_2018, sent2_2019), axis=-1)\n",
    "    print('Image stack:', image_stack.shape)\n",
    "    del sent2_2018, sent2_2019\n",
    "\n",
    "if img_type == 'S1':\n",
    "    # Load images\n",
    "    sar_2018_vh = np.expand_dims(load_SAR_image(root_path+'New_Images/Sentinel1/'+'cut_sent1_vh_2018.tif').astype('float32'), axis = -1)\n",
    "    sar_2018_vv = np.expand_dims(load_SAR_image(root_path+'New_Images/Sentinel1/'+'cut_sent1_vv_2018.tif').astype('float32'), axis = -1)\n",
    "    sar_2019_vh = np.expand_dims(load_SAR_image(root_path+'New_Images/Sentinel1/'+'cut_sent1_vh_2019.tif').astype('float32'), axis = -1)\n",
    "    sar_2019_vv = np.expand_dims(load_SAR_image(root_path+'New_Images/Sentinel1/'+'cut_sent1_vv_2019.tif').astype('float32'), axis = -1)\n",
    "    \n",
    "    sar_2018 = np.concatenate((sar_2018_vh, sar_2018_vv), axis=-1)\n",
    "    sar_2019 = np.concatenate((sar_2019_vh, sar_2019_vv), axis=-1)\n",
    "    del sar_2018_vh, sar_2018_vv, sar_2019_vh, sar_2019_vv\n",
    "    \n",
    "    # Filter outliers\n",
    "    sar_2018 = filter_outliers(sar_2018.copy()) \n",
    "    sar_2019 = filter_outliers(sar_2019.copy()) \n",
    "\n",
    "    image_stack = np.concatenate((sar_2018, sar_2019), axis=-1)\n",
    "    print('Image stack:', image_stack.shape)\n",
    "    del sar_2018, sar_2019\n",
    "\n",
    "if img_type == 'FUSION':\n",
    "    # Load images\n",
    "    sent2_2018_1 = load_tif_image(root_path+'New_Images/Sentinel2/'+'2018_10m_b2348.tif').astype('float32')\n",
    "    #sent2_2018_2 = load_tif_image(root_path+'New_Images/Sentinel2/'+'2018_20m_b5678a1112.tif').astype('float32')\n",
    "    \n",
    "    # Resize bands of 20m\n",
    "    #sent2_2018_2 = resize_image(sent2_2018_2.copy(), sent2_2018_1.shape[0], sent2_2018_1.shape[1])\n",
    "    #sent2_2018 = np.concatenate((sent2_2018_1, sent2_2018_2), axis=-1)\n",
    "    sent2_2018 = sent2_2018_1.copy()\n",
    "    del sent2_2018_1#, sent2_2018_2\n",
    "    \n",
    "    sent2_2019_1 = load_tif_image(root_path+'New_Images/Sentinel2/'+'2019_10m_b2348.tif').astype('float32')\n",
    "    #sent2_2019_2 = load_tif_image(root_path+'New_Images/Sentinel2/'+'2019_20m_b5678a1112.tif').astype('float32')   \n",
    "    \n",
    "    # Resize bands of 20m\n",
    "    #sent2_2019_2 = resize_image(sent2_2019_2.copy(), sent2_2019_1.shape[0], sent2_2019_1.shape[1])\n",
    "    #sent2_2019 = np.concatenate((sent2_2019_1, sent2_2019_2), axis=-1)\n",
    "    sent2_2019 = sent2_2019_1.copy()\n",
    "    del sent2_2019_1#, sent2_2019_2\n",
    "    \n",
    "    # Filter outliers\n",
    "    sent2_2018 = filter_outliers(sent2_2018.copy()) \n",
    "    sent2_2019 = filter_outliers(sent2_2019.copy()) \n",
    "    \n",
    "    opt_image_stack = np.concatenate((sent2_2018, sent2_2019), axis=-1)\n",
    "    #print('Image stack:', image_stack.shape)\n",
    "    \n",
    "\n",
    "    # Load images\n",
    "    sar_2018_vh = np.expand_dims(load_SAR_image(root_path+'New_Images/Sentinel1/'+'cut_sent1_vh_2018.tif').astype('float32'), axis = -1)\n",
    "    sar_2018_vv = np.expand_dims(load_SAR_image(root_path+'New_Images/Sentinel1/'+'cut_sent1_vv_2018.tif').astype('float32'), axis = -1)\n",
    "    sar_2019_vh = np.expand_dims(load_SAR_image(root_path+'New_Images/Sentinel1/'+'cut_sent1_vh_2019.tif').astype('float32'), axis = -1)\n",
    "    sar_2019_vv = np.expand_dims(load_SAR_image(root_path+'New_Images/Sentinel1/'+'cut_sent1_vv_2019.tif').astype('float32'), axis = -1)\n",
    "    \n",
    "    sar_2018 = np.concatenate((sar_2018_vh, sar_2018_vv), axis=-1)\n",
    "    sar_2019 = np.concatenate((sar_2019_vh, sar_2019_vv), axis=-1)\n",
    "    del sar_2018_vh, sar_2018_vv, sar_2019_vh, sar_2019_vv\n",
    "    \n",
    "    # Filter outliers\n",
    "    sar_2018 = filter_outliers(sar_2018.copy()) \n",
    "    sar_2019 = filter_outliers(sar_2019.copy()) \n",
    "\n",
    "    sar_image_stack = np.concatenate((sar_2018, sar_2019), axis=-1)\n",
    "    #print('Image stack:', image_stack.shape)\n",
    "    del sar_2018, sar_2019\n",
    "    del sent2_2018, sent2_2019\n",
    "    \n",
    "    image_stack = np.concatenate((opt_image_stack, sar_image_stack), axis=-1)\n",
    "    print('Image stack:', image_stack.shape)\n",
    "    \n",
    "\n",
    "# load references     \n",
    "# Load current reference \n",
    "#ref_2019 = load_tif_image(root_path+'New_Images/References/res_10m/r10m_def_2019.tif').astype('float32') # actual 2019\n",
    "# Load past references\n",
    "#past_ref = np.load(root_path+'New_Images/References/past_ref_and_clouds.npy').astype('float32')\n",
    "#past_ref1 = load_tif_image(root_path+'New_Images/References/res_10m/r10m_def_1988_2007.tif').astype('float32') # 1988_2007\n",
    "#past_ref2 = load_tif_image(root_path+'New_Images/References/res_10m/r10m_def_2008_2018.tif').astype('float32') # 2008_2018\n",
    "#clouds_2018 = load_tif_image(root_path+'New_Images/References/cut_b10_2018.tif').astype('float32')\n",
    "#clouds_2018 = resize_image(np.expand_dims(clouds_2018.copy(), axis = -1), ref_2019.shape[0], ref_2019.shape[1])\n",
    "#clouds_2018 = binary_mask_cloud(clouds_2018.copy(), 50)\n",
    "#clouds_2019 = load_tif_image(root_path+'New_Images/References/cut_b10_2019.tif').astype('float32') \n",
    "#clouds_2019 = resize_image(np.expand_dims(clouds_2019.copy(), axis = -1), ref_2019.shape[0], ref_2019.shape[1])\n",
    "#clouds_2019 = binary_mask_cloud(clouds_2019.copy(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PrXGBIiimC0"
   },
   "outputs": [],
   "source": [
    "# Create label mask\n",
    "#past_ref = past_ref1 + past_ref2 + clouds_2018 + clouds_2019\n",
    "#past_ref[past_ref>=1] = 1\n",
    "#buffer = 2\n",
    "#final_mask1 = mask_no_considered(ref_2019, buffer, past_ref)\n",
    "#del past_ref1, past_ref2, clouds_2018, clouds_2019\n",
    "final_mask1 = np.load(root_path+'New_Images/ref/'+'labels.npy')\n",
    "\n",
    "lim_x = 10000\n",
    "lim_y = 7000\n",
    "image_stack = image_stack[:lim_x, :lim_y, :]\n",
    "final_mask1 = final_mask1[:lim_x, :lim_y]\n",
    "#ref_2019 = ref_2019[:lim_x, :lim_y]\n",
    "\n",
    "h_, w_, channels = image_stack.shape\n",
    "print('image stack size: ', image_stack.shape)\n",
    "\n",
    "# Normalization\n",
    "type_norm = 1\n",
    "image_array = normalization(image_stack.copy(), type_norm)\n",
    "print(np.min(image_array), np.max(image_array))\n",
    "del image_stack\n",
    "\n",
    "# Print pertengate of each class (whole image)\n",
    "print('Total no-deforestaion class is {}'.format(len(final_mask1[final_mask1==0])))\n",
    "print('Total deforestaion class is {}'.format(len(final_mask1[final_mask1==1])))\n",
    "print('Total past deforestaion class is {}'.format(len(final_mask1[final_mask1==2])))\n",
    "print('Percentage of deforestaion class is {:.2f}'.format((len(final_mask1[final_mask1==1])*100)/len(final_mask1[final_mask1==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knelmZc2HRTO"
   },
   "outputs": [],
   "source": [
    "# Create tile mask\n",
    "mask_tiles = create_mask(final_mask1.shape[0], final_mask1.shape[1], grid_size=(5, 4))\n",
    "image_array = image_array[:mask_tiles.shape[0], :mask_tiles.shape[1],:]\n",
    "final_mask1 = final_mask1[:mask_tiles.shape[0], :mask_tiles.shape[1]]\n",
    "\n",
    "print('mask: ',mask_tiles.shape)\n",
    "print('image stack: ', image_array.shape)\n",
    "print('ref :', final_mask1.shape)\n",
    "#plt.imshow(mask_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gExq8XOCckvb"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(final_mask1, cmap = 'jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKaXQmmGoxLz"
   },
   "outputs": [],
   "source": [
    "# Define tiles for training, validation, and test sets\n",
    "tiles_tr = [1,3,5,8,11,13,14,20]\n",
    "tiles_val = [6,19]\n",
    "tiles_ts = (list(set(np.arange(20)+1)-set(tiles_tr)-set(tiles_val)))\n",
    "\n",
    "mask_tr_val = np.zeros((mask_tiles.shape)).astype('float32')\n",
    "# Training and validation mask\n",
    "for tr_ in tiles_tr:\n",
    "    mask_tr_val[mask_tiles == tr_] = 1\n",
    "\n",
    "for val_ in tiles_val:\n",
    "    mask_tr_val[mask_tiles == val_] = 2\n",
    "\n",
    "mask_amazon_ts = np.zeros((mask_tiles.shape)).astype('float32')\n",
    "for ts_ in tiles_ts:\n",
    "    mask_amazon_ts[mask_tiles == ts_] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QI8Gc-NQrr44"
   },
   "outputs": [],
   "source": [
    "# Create ixd image to extract patches\n",
    "overlap = 0.7\n",
    "patch_size = 128\n",
    "batch_size = 32\n",
    "im_idx = create_idx_image(final_mask1)\n",
    "patches_idx = extract_patches(im_idx, patch_size=(patch_size, patch_size), overlap=overlap).reshape(-1,patch_size, patch_size)\n",
    "patches_mask = extract_patches(mask_tr_val, patch_size=(patch_size, patch_size), overlap=overlap).reshape(-1, patch_size, patch_size)\n",
    "del im_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1V39Gf4owRBa"
   },
   "outputs": [],
   "source": [
    "# Selecting index trn val and test patches idx\n",
    "idx_trn = np.squeeze(np.where(patches_mask.sum(axis=(1, 2))==patch_size**2))\n",
    "idx_val = np.squeeze(np.where(patches_mask.sum(axis=(1, 2))==2*patch_size**2))\n",
    "del patches_mask\n",
    "\n",
    "patches_idx_trn = patches_idx[idx_trn]\n",
    "patches_idx_val = patches_idx[idx_val]\n",
    "del idx_trn, idx_val\n",
    "\n",
    "print('Number of training patches:  ', len(patches_idx_trn), 'Number of validation patches', len(patches_idx_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycpxzOJblJvr"
   },
   "outputs": [],
   "source": [
    "# Extract patches with at least 2% of deforestation class\n",
    "X_train = retrieve_idx_percentage(final_mask1, patches_idx_trn, patch_size, pertentage = 2)\n",
    "X_valid = retrieve_idx_percentage(final_mask1, patches_idx_val, patch_size, pertentage = 2)\n",
    "print(X_train.shape, X_valid.shape)\n",
    "del patches_idx_trn, patches_idx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpchvB9nZOmC"
   },
   "outputs": [],
   "source": [
    "def batch_generator(batches, image, reference, target_size, number_class):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    image = image.reshape(-1, image.shape[-1])\n",
    "    reference = reference.reshape(final_mask1.shape[0]*final_mask1.shape[1])\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_x = np.squeeze(batch_x.astype('int64'))\n",
    "        #print(batch_x.shape)\n",
    "        batch_img = np.zeros((batch_x.shape[0], target_size, target_size, image.shape[-1]))\n",
    "        batch_ref = np.zeros((batch_x.shape[0], target_size, target_size, number_class))\n",
    "        \n",
    "        for i in range(batch_x.shape[0]):\n",
    "            if np.random.rand()>0.5:\n",
    "                batch_x[i] = np.rot90(batch_x[i], 1)\n",
    "            batch_img[i] = image[batch_x[i]] \n",
    "            batch_ref[i] = tf.keras.utils.to_categorical(reference[batch_x[i]] , number_class)\n",
    "                       \n",
    "        yield (batch_img, batch_ref)\n",
    "\n",
    "train_datagen = ImageDataGenerator(horizontal_flip = True,\n",
    "                                   vertical_flip = True)\n",
    "valid_datagen = ImageDataGenerator(horizontal_flip = True, \n",
    "                                   vertical_flip = True)\n",
    "\n",
    "y_train = np.zeros((len(X_train)))\n",
    "y_valid = np.zeros((len(X_valid)))\n",
    "\n",
    "train_gen = train_datagen.flow(np.expand_dims(X_train, axis = -1), y_train,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_gen = valid_datagen.flow(np.expand_dims(X_valid, axis = -1), y_valid,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "number_class = 3\n",
    "train_gen_crops = batch_generator(train_gen, image_array, final_mask1, patch_size, number_class)\n",
    "valid_gen_crops = batch_generator(valid_gen, image_array, final_mask1, patch_size, number_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfJK-atSgFTG"
   },
   "outputs": [],
   "source": [
    "exp = 1\n",
    "path_exp = root_path+'experiments/exp'+str(exp)\n",
    "path_models = path_exp+'/models'\n",
    "path_maps = path_exp+'/pred_maps'\n",
    "\n",
    "if not os.path.exists(path_exp):\n",
    "    os.makedirs(path_exp)   \n",
    "if not os.path.exists(path_models):\n",
    "    os.makedirs(path_models)   \n",
    "if not os.path.exists(path_maps):\n",
    "    os.makedirs(path_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ol_1Ci3TlbrM"
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "input_shape = (patch_size, patch_size, channels)\n",
    "nb_filters = [32, 64, 128]\n",
    "\n",
    "method = 'unet'\n",
    "if method == 'unet':\n",
    "   model = build_unet(input_shape, nb_filters, number_class)\n",
    "\n",
    "if method == 'resunet':\n",
    "   model = build_resunet(input_shape, nb_filters, number_class)\n",
    "\n",
    "model = Model_3(nb_filters, number_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4Ql8N-flPbi"
   },
   "outputs": [],
   "source": [
    "# Parameters of the model\n",
    "weights = [0.2, 0.8, 0]\n",
    "adam = Adam(lr = 1e-3 , beta_1=0.9)\n",
    "loss = weighted_categorical_crossentropy(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "568DB3nZLrHC"
   },
   "outputs": [],
   "source": [
    "metrics_all = []\n",
    "times=5\n",
    "for tm in range(0,times):\n",
    "    print('time: ', tm)\n",
    "\n",
    "    rows = patch_size\n",
    "    cols = patch_size\n",
    "    adam = Adam(lr = 1e-3 , beta_1=0.9)\n",
    "    \n",
    "    loss = weighted_categorical_crossentropy(weights)\n",
    "\n",
    "    #if method == 'unet':\n",
    "    #   model = build_unet(input_shape, nb_filters, number_class)\n",
    "\n",
    "    #if method == 'resunet':\n",
    "    #   model = build_resunet(input_shape, nb_filters, number_class)\n",
    "    \n",
    "    model = Model_3(nb_filters, number_class)\n",
    "    model.build((None,)+input_shape)\n",
    "    \n",
    "    model.compile(optimizer=adam, loss=loss, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, mode='min')\n",
    "    #earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, mode='min') ---- val_accuracy\n",
    "    #checkpoint = ModelCheckpoint(path_models+ '/' + method +'_'+str(tm)+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    checkpoint = ModelCheckpoint(path_models+ '/' + method +'_'+str(tm)+'.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
    "    lr_reduce = ReduceLROnPlateau(factor=0.9, min_delta=0.0001, patience=5, verbose=1)\n",
    "    callbacks_list = [earlystop, checkpoint]\n",
    "    # train the model\n",
    "    start_training = time.time()\n",
    "    history = model.fit(train_gen_crops,\n",
    "                              steps_per_epoch=len(X_train)*3//train_gen.batch_size,\n",
    "                              validation_data=valid_gen_crops,\n",
    "                              validation_steps=len(X_valid)*3//valid_gen.batch_size,\n",
    "                              epochs=100,\n",
    "                              callbacks=callbacks_list)\n",
    "    end_training = time.time() - start_training\n",
    "    metrics_all.append(end_training)\n",
    "    del model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMem2rkfpL-g"
   },
   "outputs": [],
   "source": [
    "# Test loop\n",
    "time_ts = []\n",
    "n_pool = 3\n",
    "n_rows = 5\n",
    "n_cols = 4\n",
    "rows, cols = image_array.shape[:2]\n",
    "pad_rows = rows - np.ceil(rows/(n_rows*2**n_pool))*n_rows*2**n_pool\n",
    "pad_cols = cols - np.ceil(cols/(n_cols*2**n_pool))*n_cols*2**n_pool\n",
    "print(pad_rows, pad_cols)\n",
    "\n",
    "npad = ((0, int(abs(pad_rows))), (0, int(abs(pad_cols))), (0, 0))\n",
    "image1_pad = np.pad(image_array, pad_width=npad, mode='reflect')\n",
    "\n",
    "h, w, c = image1_pad.shape\n",
    "patch_size_rows = h//n_rows\n",
    "patch_size_cols = w//n_cols\n",
    "num_patches_x = int(h/patch_size_rows)\n",
    "num_patches_y = int(w/patch_size_cols)\n",
    "\n",
    "input_shape=(patch_size_rows,patch_size_cols, c)\n",
    "\n",
    "#if method == 'unet':\n",
    "#   new_model = build_unet(input_shape, nb_filters, number_class)\n",
    "\n",
    "#if method == 'resunet':\n",
    "#   new_model = build_resunet(input_shape, nb_filters, number_class)\n",
    "\n",
    "new_model = Model_3(nb_filters, number_class)\n",
    "new_model.build((None,)+input_shape)\n",
    "\n",
    "for tm in range(0,times):\n",
    "    print('time: ', tm)\n",
    "    #model = load_model(path_models+ '/' + method +'_'+str(tm)+'.h5', compile=False)\n",
    "    \n",
    "    #for l in range(1, len(model.layers)):\n",
    "    #    new_model.layers[l].set_weights(model.layers[l].get_weights())\n",
    "    new_model.load_weights(path_models+ '/' + method +'_'+str(tm)+'.h5')\n",
    "    \n",
    "    start_test = time.time()\n",
    "    patch_t = []\n",
    "    \n",
    "    for i in range(0,num_patches_y):\n",
    "        for j in range(0,num_patches_x):\n",
    "            patch = image1_pad[patch_size_rows*j:patch_size_rows*(j+1), patch_size_cols*i:patch_size_cols*(i+1), :]\n",
    "            predictions_ = new_model.predict(np.expand_dims(patch, axis=0))\n",
    "            del patch \n",
    "            patch_t.append(predictions_[:,:,:,1])\n",
    "            del predictions_\n",
    "    end_test =  time.time() - start_test\n",
    "    patches_pred = np.asarray(patch_t).astype(np.float32)\n",
    "\n",
    "    prob_recontructed = pred_reconctruct(h, w, num_patches_x, num_patches_y, patch_size_rows, patch_size_cols, patches_pred)\n",
    "    np.save(path_maps+'/'+'prob_'+str(tm)+'.npy',prob_recontructed) \n",
    "\n",
    "    time_ts.append(end_test)\n",
    "    del prob_recontructed, patches_pred\n",
    "    #del model\n",
    "time_ts_array = np.asarray(time_ts)\n",
    "# Save test time\n",
    "np.save(path_exp+'/metrics_ts.npy', time_ts_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ehh68acZW2lR"
   },
   "outputs": [],
   "source": [
    "# Compute mean of the tm predictions maps\n",
    "prob_rec = np.zeros((image1_pad.shape[0],image1_pad.shape[1], times))\n",
    "\n",
    "for tm in range (0, times):\n",
    "    print(tm)\n",
    "    prob_rec[:,:,tm] = np.load(path_maps+'/'+'prob_'+str(tm)+'.npy').astype(np.float32)\n",
    "\n",
    "mean_prob = np.mean(prob_rec, axis = -1)\n",
    "np.save(path_maps+'/prob_mean.npy', mean_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVDldxUWckvg"
   },
   "outputs": [],
   "source": [
    "# Plot mean map and reference\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax1 = fig.add_subplot(121)\n",
    "plt.title('Prediction')\n",
    "ax1.imshow(mean_prob, cmap ='jet')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "plt.title('Reference')\n",
    "ax2.imshow(final_mask1, cmap ='jet')\n",
    "ax2.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4P8pVKDxW6Lh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Computing metrics\n",
    "mean_prob = mean_prob[:final_mask1.shape[0], :final_mask1.shape[1]]\n",
    "ref1 = np.ones_like(final_mask1).astype(np.float32)\n",
    "\n",
    "ref1 [final_mask1 == 2] = 0\n",
    "TileMask = mask_amazon_ts * ref1\n",
    "GTTruePositives = final_mask1==1\n",
    "    \n",
    "Npoints = 10\n",
    "Pmax = np.max(mean_prob[GTTruePositives * TileMask ==1])\n",
    "ProbList = np.linspace(Pmax,0,Npoints)\n",
    "    \n",
    "metrics_ = matrics_AA_recall(ProbList, mean_prob, final_mask1, mask_amazon_ts, 625)\n",
    "np.save(path_exp+'/acc_metrics.npy',metrics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DB-vr1sq6PwK"
   },
   "outputs": [],
   "source": [
    "# Complete NaN values\n",
    "metrics_copy = metrics_.copy()\n",
    "metrics_copy = complete_nan_values(metrics_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HNG_I2cW8JF"
   },
   "outputs": [],
   "source": [
    "# Comput Mean Average Precision (mAP) score \n",
    "Recall = metrics_copy[:,0]\n",
    "Precision = metrics_copy[:,1]\n",
    "AA = metrics_copy[:,2]\n",
    "    \n",
    "DeltaR = Recall[1:]-Recall[:-1]\n",
    "AP = np.sum(Precision[:-1]*DeltaR)\n",
    "print('mAP', AP)\n",
    "\n",
    "# Plot Recall vs. Precision curve\n",
    "plt.close('all')\n",
    "plt.plot(metrics_copy[:,0],metrics_copy[:,1])\n",
    "plt.plot(metrics_copy[:,0],metrics_copy[:,2])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "U-Net and Res-Unet tf2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
