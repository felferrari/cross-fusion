{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SYkAbHbfcku-"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2L_yKE1IgpA0"
   },
   "outputs": [],
   "source": [
    "#%autoreload # When utils.py is updated\n",
    "#from utils_unet_resunet import *\n",
    "from ops import pred_reconctruct\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from model.callbacks import UpdateWeights\n",
    "#from model.models import UNET\n",
    "from model.losses import WBCE\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import importlib\n",
    "import numpy as np\n",
    "#from utils_unet_resunet import build_unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'00_params.json') as param_file:\n",
    "    params = json.load(param_file)\n",
    "\n",
    "img_path = 'imgs' \n",
    "n_opt_layer = 26 #number of OPT layers, used to split de input data between OPT and SAR\n",
    "\n",
    "number_class = 3\n",
    "weights = params['weights']\n",
    "times= params['times']\n",
    "exp = params['exp']\n",
    "\n",
    "overlap = params['overlap']\n",
    "patch_size = params['patch_size']\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "img_type = params['img_type']\n",
    "cond = params['cond']\n",
    "test_cond = params['test_cond']\n",
    "\n",
    "nb_filters = params['nb_filters']\n",
    "\n",
    "method = params['method']\n",
    "module = importlib.import_module('model.models')\n",
    "exp_model = getattr(module, method)\n",
    "\n",
    "grid_size = params['grid_size']\n",
    "\n",
    "tiles_tr = params['tiles_tr']\n",
    "tiles_val = params['tiles_val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kylt2BueckvP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image stack: (17730, 9203, 26)\n"
     ]
    }
   ],
   "source": [
    "image_array = np.load(os.path.join(img_path, f'fus_stack_{test_cond}.npy'))\n",
    "if img_type == 'OPT':\n",
    "    image_array = image_array[:, :, :n_opt_layer]\n",
    "    \n",
    "if img_type == 'SAR':\n",
    "    image_array = image_array[:, :, n_opt_layer:]\n",
    "    \n",
    "    \n",
    "print('Image stack:', image_array.shape)\n",
    "h_, w_, channels = image_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tiles mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QfJK-atSgFTG"
   },
   "outputs": [],
   "source": [
    "path_exp = os.path.join(img_path, 'experiments', f'exp_{exp}')\n",
    "path_models = os.path.join(path_exp, 'models')\n",
    "path_maps = os.path.join(path_exp, 'pred_maps')\n",
    "\n",
    "if not os.path.exists(path_exp):\n",
    "    os.makedirs(path_exp)   \n",
    "if not os.path.exists(path_models):\n",
    "    os.makedirs(path_models)   \n",
    "if not os.path.exists(path_maps):\n",
    "    os.makedirs(path_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions mapsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nMem2rkfpL-g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-30.0 -13.0\n",
      "model 0: 19.48\n",
      "model 1: 14.69\n",
      "model 2: 14.73\n",
      "model 3: 14.63\n",
      "model 4: 14.54\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (patch_size, patch_size, channels)\n",
    "time_ts = []\n",
    "n_pool = 3\n",
    "n_rows = 12#6\n",
    "n_cols = 6#3\n",
    "rows, cols = image_array.shape[:2]\n",
    "pad_rows = rows - np.ceil(rows/(n_rows*2**n_pool))*n_rows*2**n_pool\n",
    "pad_cols = cols - np.ceil(cols/(n_cols*2**n_pool))*n_cols*2**n_pool\n",
    "print(pad_rows, pad_cols)\n",
    "\n",
    "npad = ((0, int(abs(pad_rows))), (0, int(abs(pad_cols))), (0, 0))\n",
    "image1_pad = np.pad(image_array, pad_width=npad, mode='reflect')\n",
    "\n",
    "h, w, c = image1_pad.shape\n",
    "patch_size_rows = h//n_rows\n",
    "patch_size_cols = w//n_cols\n",
    "num_patches_x = int(h/patch_size_rows)\n",
    "num_patches_y = int(w/patch_size_cols)\n",
    "\n",
    "input_shape=(patch_size_rows,patch_size_cols, c)\n",
    "\n",
    "new_model = exp_model(nb_filters, number_class)\n",
    "new_model.build((None,)+input_shape)\n",
    "#new_model = build_unet(input_shape, nb_filters, number_class)\n",
    "\n",
    "\n",
    "adam = Adam(lr = 1e-3 , beta_1=0.9)\n",
    "loss = WBCE(weights = weights)\n",
    "#loss = weighted_categorical_crossentropy(weights)\n",
    "new_model.compile(optimizer=adam, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "for tm in range(0,times):\n",
    "    #print('time: ', tm)\n",
    "    \n",
    "    new_model.load_weights(os.path.join(path_models, f'{method}_{tm}.h5'))\n",
    "    \n",
    "    #new_model = build_unet(input_shape, nb_filters, number_class)\n",
    "    #model = build_unet(input_shape, nb_filters, number_class)\n",
    "    #model.load_weights(os.path.join(path_models, f'{method}_{tm}.h5'))\n",
    "    #for l in range(1, len(model.layers)):\n",
    "    #    new_model.layers[l].set_weights(model.layers[l].get_weights())\n",
    "    \n",
    "    start_test = time.time()\n",
    "    patch_list = []\n",
    "\n",
    "    \n",
    "    for i in range(0,num_patches_y):\n",
    "        for j in range(0,num_patches_x):\n",
    "            patch = image1_pad[patch_size_rows*j:patch_size_rows*(j+1), patch_size_cols*i:patch_size_cols*(i+1), :]\n",
    "            pred = new_model.predict(np.expand_dims(patch, axis=0))\n",
    "            del patch \n",
    "            patch_list.append(pred[:,:,:,1])\n",
    "\n",
    "            del pred\n",
    "    end_test =  time.time() - start_test\n",
    "\n",
    "    patches_pred = np.asarray(patch_list).astype(np.float32)\n",
    "    \n",
    "    del patch_list\n",
    "\n",
    "    prob_recontructed = pred_reconctruct(h, w, num_patches_x, num_patches_y, patch_size_rows, patch_size_cols, patches_pred)\n",
    "    \n",
    "    del patches_pred\n",
    "    np.save(os.path.join(path_maps, f'prob_{tm}.npy'),prob_recontructed) \n",
    "\n",
    "    time_ts.append(end_test)\n",
    "    print(f'model {tm}: {end_test:.2f}')\n",
    "    del prob_recontructed\n",
    "    gc.collect()\n",
    "    #del model\n",
    "time_ts_array = np.asarray(time_ts)\n",
    "# Save test time\n",
    "np.save(os.path.join(path_exp, 'metrics_ts.npy'), time_ts_array)\n",
    "del image_array\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the mean of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ehh68acZW2lR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Compute mean of the tm predictions maps\n",
    "prob_rec = np.zeros((image1_pad.shape[0],image1_pad.shape[1], times))\n",
    "\n",
    "for tm in range (0, times):\n",
    "    print(tm)\n",
    "    prob_rec[:,:,tm] = np.load(os.path.join(path_maps, f'prob_{tm}.npy')).astype(np.float32)\n",
    "\n",
    "mean_prob = np.mean(prob_rec, axis = -1)\n",
    "\n",
    "np.save(os.path.join(path_maps, 'prob_mean.npy'), mean_prob)\n",
    "\n",
    "for tm in range (0, times):\n",
    "    os.remove(os.path.join(path_maps, f'prob_{tm}.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "U-Net and Res-Unet tf2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b08b86eefb6f9027df8c705c57ad3330ee722a1038604439ab9df613faded208"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
