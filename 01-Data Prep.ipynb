{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SYkAbHbfcku-"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2L_yKE1IgpA0"
   },
   "outputs": [],
   "source": [
    "#%autoreload # When utils.py is updated\n",
    "from utils_unet_resunet import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from model.models import Model_3\n",
    "import numpy as np\n",
    "root_path = 'imgs/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kylt2BueckvP",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs/New_Images/Sentinel2/2018_10m_b2348.tif\n",
      "imgs/New_Images/Sentinel2/2018_20m_b5678a1112.tif\n",
      "imgs/New_Images/Sentinel2/2019_10m_b2348.tif\n",
      "imgs/New_Images/Sentinel2/2019_20m_b5678a1112.tif\n",
      "imgs/New_Images/Sentinel1/cut_sent1_vh_2018.tif\n",
      "imgs/New_Images/Sentinel1/cut_sent1_vv_2018.tif\n",
      "imgs/New_Images/Sentinel1/cut_sent1_vh_2019.tif\n",
      "imgs/New_Images/Sentinel1/cut_sent1_vv_2019.tif\n",
      "-5.969623 5.626766\n",
      "-1.3673552 6.424306\n",
      "Image stack: (10000, 7000, 24)\n"
     ]
    }
   ],
   "source": [
    "# Define data type (L8-Landsat8, S2-Sentinel2, S1-Sentinel1)\n",
    "\n",
    "\n",
    "sent2_2018_1 = load_tif_image(root_path+'New_Images/Sentinel2/'+'2018_10m_b2348.tif').astype('float32')\n",
    "sent2_2018_2 = load_tif_image(root_path+'New_Images/Sentinel2/'+'2018_20m_b5678a1112.tif').astype('float32')\n",
    "\n",
    "# Resize bands of 20m\n",
    "sent2_2018_2 = resize_image(sent2_2018_2.copy(), sent2_2018_1.shape[0], sent2_2018_1.shape[1])\n",
    "sent2_2018 = np.concatenate((sent2_2018_1, sent2_2018_2), axis=-1)\n",
    "#sent2_2018 = sent2_2018_1.copy()\n",
    "del sent2_2018_1, sent2_2018_2\n",
    "\n",
    "sent2_2019_1 = load_tif_image(root_path+'New_Images/Sentinel2/'+'2019_10m_b2348.tif').astype('float32')\n",
    "sent2_2019_2 = load_tif_image(root_path+'New_Images/Sentinel2/'+'2019_20m_b5678a1112.tif').astype('float32')   \n",
    "\n",
    "# Resize bands of 20m\n",
    "sent2_2019_2 = resize_image(sent2_2019_2.copy(), sent2_2019_1.shape[0], sent2_2019_1.shape[1])\n",
    "sent2_2019 = np.concatenate((sent2_2019_1, sent2_2019_2), axis=-1)\n",
    "#sent2_2019 = sent2_2019_1.copy()\n",
    "del sent2_2019_1, sent2_2019_2\n",
    "\n",
    "# Filter outliers\n",
    "sent2_2018 = filter_outliers(sent2_2018.copy()) \n",
    "sent2_2019 = filter_outliers(sent2_2019.copy()) \n",
    "\n",
    "opt_image_stack = np.concatenate((sent2_2018, sent2_2019), axis=-1)\n",
    "\n",
    "\n",
    "# Load images\n",
    "sar_2018_vh = np.expand_dims(load_SAR_image(root_path+'New_Images/Sentinel1/'+'cut_sent1_vh_2018.tif').astype('float32'), axis = -1)\n",
    "sar_2018_vv = np.expand_dims(load_SAR_image(root_path+'New_Images/Sentinel1/'+'cut_sent1_vv_2018.tif').astype('float32'), axis = -1)\n",
    "sar_2019_vh = np.expand_dims(load_SAR_image(root_path+'New_Images/Sentinel1/'+'cut_sent1_vh_2019.tif').astype('float32'), axis = -1)\n",
    "sar_2019_vv = np.expand_dims(load_SAR_image(root_path+'New_Images/Sentinel1/'+'cut_sent1_vv_2019.tif').astype('float32'), axis = -1)\n",
    "\n",
    "sar_2018 = np.concatenate((sar_2018_vh, sar_2018_vv), axis=-1)\n",
    "sar_2019 = np.concatenate((sar_2019_vh, sar_2019_vv), axis=-1)\n",
    "del sar_2018_vh, sar_2018_vv, sar_2019_vh, sar_2019_vv\n",
    "\n",
    "# Filter outliers\n",
    "sar_2018 = filter_outliers(sar_2018.copy()) \n",
    "sar_2019 = filter_outliers(sar_2019.copy()) \n",
    "\n",
    "sar_image_stack = np.concatenate((sar_2018, sar_2019), axis=-1)\n",
    "del sar_2018, sar_2019\n",
    "del sent2_2018, sent2_2019\n",
    "\n",
    "\n",
    "#masking\n",
    "final_mask1 = np.load(root_path+'New_Images/ref/'+'labels.npy')\n",
    "lim_x = 10000\n",
    "lim_y = 7000\n",
    "opt_image_stack = opt_image_stack[:lim_x, :lim_y, :]\n",
    "sar_image_stack = sar_image_stack[:lim_x, :lim_y, :]\n",
    "final_mask1 = final_mask1[:lim_x, :lim_y]\n",
    "\n",
    "#normalizing\n",
    "type_norm = 1\n",
    "opt_image_array = normalization(opt_image_stack.copy(), type_norm)\n",
    "del opt_image_stack\n",
    "print(np.min(opt_image_array), np.max(opt_image_array))\n",
    "sar_image_array = normalization(sar_image_stack.copy(), type_norm)\n",
    "del sar_image_stack\n",
    "print(np.min(sar_image_array), np.max(sar_image_array))\n",
    "\n",
    "#concatenate SAR and OPT\n",
    "image_stack = np.concatenate((opt_image_array, sar_image_array), axis=-1)\n",
    "print('Image stack:', image_stack.shape)\n",
    "\n",
    "\n",
    "np.save(root_path+'New_Images/opt_stack.npy', opt_image_array)\n",
    "np.save(root_path+'New_Images/sar_stack.npy', sar_image_array)\n",
    "np.save(root_path+'New_Images/fus_stack.npy', image_stack)\n",
    "np.save(root_path+'New_Images/final_mask1.npy', final_mask1)\n",
    "\n",
    "del image_stack\n",
    "\n",
    "# load references     \n",
    "# Load current reference \n",
    "#ref_2019 = load_tif_image(root_path+'New_Images/References/res_10m/r10m_def_2019.tif').astype('float32') # actual 2019\n",
    "# Load past references\n",
    "#past_ref = np.load(root_path+'New_Images/References/past_ref_and_clouds.npy').astype('float32')\n",
    "#past_ref1 = load_tif_image(root_path+'New_Images/References/res_10m/r10m_def_1988_2007.tif').astype('float32') # 1988_2007\n",
    "#past_ref2 = load_tif_image(root_path+'New_Images/References/res_10m/r10m_def_2008_2018.tif').astype('float32') # 2008_2018\n",
    "#clouds_2018 = load_tif_image(root_path+'New_Images/References/cut_b10_2018.tif').astype('float32')\n",
    "#clouds_2018 = resize_image(np.expand_dims(clouds_2018.copy(), axis = -1), ref_2019.shape[0], ref_2019.shape[1])\n",
    "#clouds_2018 = binary_mask_cloud(clouds_2018.copy(), 50)\n",
    "#clouds_2019 = load_tif_image(root_path+'New_Images/References/cut_b10_2019.tif').astype('float32') \n",
    "#clouds_2019 = resize_image(np.expand_dims(clouds_2019.copy(), axis = -1), ref_2019.shape[0], ref_2019.shape[1])\n",
    "#clouds_2019 = binary_mask_cloud(clouds_2019.copy(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no-deforestaion class is 36326397\n",
      "Total deforestaion class is 1048775\n",
      "Total past deforestaion class is 32624828\n",
      "Percentage of deforestaion class is 2.89\n"
     ]
    }
   ],
   "source": [
    "# Print pertengate of each class (whole image)\n",
    "print('Total no-deforestaion class is {}'.format(len(final_mask1[final_mask1==0])))\n",
    "print('Total deforestaion class is {}'.format(len(final_mask1[final_mask1==1])))\n",
    "print('Total past deforestaion class is {}'.format(len(final_mask1[final_mask1==2])))\n",
    "print('Percentage of deforestaion class is {:.2f}'.format((len(final_mask1[final_mask1==1])*100)/len(final_mask1[final_mask1==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "U-Net and Res-Unet tf2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b08b86eefb6f9027df8c705c57ad3330ee722a1038604439ab9df613faded208"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
